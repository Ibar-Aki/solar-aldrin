# MAX_TOKENS 900/1500 比較とエラー率改善検討レポート（多角版）

作成日時: 2026-02-11 22:32:28 +09:00  
作成者: Codex (GPT-5)

## 1. エグゼクティブサマリ
- 結論は「`MAX_TOKENS` の一律 1500 化」ではなく、「900 を既定値に据えた動的制御」が妥当です。
- 理由は、2026-02-11 の本番A/B（各 n=2）で 1500 側の改善が一貫せず、待ち時間短縮・安定化の両立が未確定なためです。
- 一方で、JSON不完全（`finish_reason=length`）に対する回復余地はあり、条件付き引き上げは有効です。
- 会話種類/場面ごとの推論時間調整は可能です。現行構成では主に `timeout/retry/max_tokens` の動的制御、将来的には Responses API + reasoningモデルで `reasoning.effort` 制御を使うのが現実的です。

## 2. 対象と前提
- 対象実装:
  - `workers/routes/chat.ts`
  - `workers/lib/openai.ts`
- 現行要点:
  - モデル: `gpt-4o-mini`
  - 通常上限: `MAX_TOKENS = 900`
  - 解析回復上限: `PARSE_RECOVERY_MAX_TOKENS = 1500`
  - `response_format: json_schema` + `strict` 相当の運用
  - リトライ/タイムアウトは環境変数で制御可能
- 実測対象（2026-02-11）:
  - 900: `reports/real-cost/LIVE/real-cost-LIVE-2026-02-11T13-08-40-132Z.md`
  - 900: `reports/real-cost/LIVE/real-cost-LIVE-2026-02-11T13-10-24-467Z.md`
  - 1500: `reports/real-cost/LIVE/real-cost-LIVE-2026-02-11T13-15-31-610Z.md`
  - 1500: `reports/real-cost/LIVE/real-cost-LIVE-2026-02-11T13-17-26-402Z.md`

## 3. 実測結果（再整理）
### 3.1 単発値
| 条件 | Total Duration | Avg API Response | Avg UI Ready | Turns | Total Tokens | Avg Tokens/Chat | OpenAI Requests | Parse Retry Used | Wait >15s |
|---|---:|---:|---:|---:|---:|---:|---:|---:|---:|
| 900 (run1) | 83.3s | 8.3s | 5.7s | 5 | 10752 | 2688 | 4 | 0 | 0 |
| 900 (run2) | 86.1s | 9.2s | 6.2s | 5 | 10877 | 2719 | 4 | 0 | 0 |
| 1500 (run1) | 46.7s | 14.1s | 7.2s | 5 | 8526 | 2842 | 3 | 0 | 1 |
| 1500 (run2) | 100.5s | 5.1s | 3.0s | 7 | 11031 | 2758 | 4 | 0 | 0 |

### 3.2 平均値（n=2）
| 条件 | Total Duration | Avg API Response | Avg UI Ready | Turns | Total Tokens | Avg Tokens/Chat | OpenAI Requests | Wait >15s |
|---|---:|---:|---:|---:|---:|---:|---:|---:|
| 900 平均 | 84.7s | 8.75s | 5.95s | 5.0 | 10814.5 | 2703.5 | 4.0 | 0.0 |
| 1500 平均 | 73.6s | 9.60s | 5.10s | 6.0 | 9778.5 | 2800.0 | 3.5 | 0.5 |

### 3.3 読み解き
- 1500 側は `Total Duration` 平均が短い一方、`Avg API Response` は悪化しています。
- 1500 側は会話ターン数が増えており（5.0→6.0）、同条件比較としてはノイズを含みます。
- `Parse Retry Used` は両条件とも 0 で、今回データから「JSON破損抑制効果」は直接評価できません。
- したがって、1500 の効果は「有望だが未確定」です。

## 4. 多角的考察
## 4.1 信頼性（JSON完全性）
- 1500 は `length` 切れを減らす方向に効く可能性があります。
- ただし JSON不完全は `length` 起因だけではありません。以下は別軸です。
  - 上流429/5xxによる欠損
  - タイムアウト中断
  - schema不一致（必須項目欠落、型不一致）
- よって「トークン増加のみ」ではエラー率全体は下がり切りません。

## 4.2 レイテンシ
- トークン上限拡大は最悪時の生成時間を伸ばすリスクがあります。
- 一方で再試行/再問い合わせを減らせれば、会話全体の所要時間は短縮し得ます。
- 実運用では「1リクエストの速さ」より「完了までの往復回数最小化」を評価軸に含めるべきです。

## 4.3 コスト
- 1500 側の `Avg Tokens/Chat` は増加（2703.5→2800.0）。
- ただし `OpenAI Requests` は減少傾向（4.0→3.5）。
- コスト最適化は「単発トークン」と「リクエスト回数」のトレードオフ設計が必要です。

## 4.4 運用リスク
- 実地A/Bで、環境変数上書き時に `OPENAI_MAX_TOKENS` が意図通り切り替わらない運用上の罠が確認されています。
- したがって、比較試験前に「現在の有効設定」を API から検証する Pre-flight は必須です。

## 4.5 UX
- ユーザー体感は「応答待ち時間のブレ」に強く影響されます。
- `Wait > 15s` を主要KPI化し、P95で管理する方がUX劣化を検知しやすいです。

## 4.6 保守性
- 例外系（認証ミス・タイムアウト・JSON不完全）の責務分離が進んでおり、改善余地は「ポリシー化」です。
- 具体的には、会話フェーズごとの共通プロファイル化（token/timeout/retry）が有効です。

## 5. 1-3対応での改善案（追記版）
## 5.1 プロンプト改善
- スキーマ不適合時の方針を system prompt に明示します。
  - 入力不足時は `null` / 空配列を優先
  - 推測で必須項目を埋めない
  - 出力は簡潔化し、冗長文を避ける
- 目的は「length回避」と「schema準拠率向上」の同時達成です。

## 5.2 パラメーター改善
- 一律設定ではなく、場面別プロファイルにします。

| プロファイル | 想定場面 | model | max_tokens | timeout | retry | temperature |
|---|---|---|---:|---:|---:|---:|
| quick | 挨拶/確認/短文応答 | gpt-4o-mini | 600-900 | 15-20s | 0-1 | 0.2-0.3 |
| standard | 通常対話（現行主軸） | gpt-4o-mini | 900 | 25s | 1 | 0.3 |
| recovery | `length` または schema異常再試行 | gpt-4o-mini | 1500 | 30-40s | 1 | 0.1-0.2 |
| deep | 複雑推論/高精度要件 | reasoningモデル | 出力上限別設定 | 40s+ | 1 | モデル推奨 |

- deep は Responses API への段階移行を前提とし、`reasoning.effort`（low/medium/high）を場面別に切替します。

## 5.3 運用・監視改善
- `meta.server.maxTokens` の事前検証を常時有効化（A/B時だけでなく定常運用でも）。
- 監視に次を追加します。
  - `finish_reason=length` 率
  - schema不一致率（`AI_RESPONSE_INVALID_SCHEMA`）
  - JSON parse失敗率（`AI_RESPONSE_INVALID_JSON`）
  - `Wait > 15s` 率
  - 上流ステータス別率（401/403/429/5xx）

## 6. 「推論時間を場面ごとに増減できるか」への詳細回答
- 可能です。実装手段は2段階あります。
- 第1段階（現行構成で即実装可）:
  - `max_tokens` / `timeout` / `retry` / `temperature` を会話種別で切替
  - 低リスク・高頻度の会話は短時間プロファイルで処理
  - 異常時のみ回復プロファイルに昇格
- 第2段階（モデル戦略の拡張）:
  - Responses API + reasoningモデルへ分岐
  - `reasoning.effort` を `low/medium/high` で制御
  - 深い推論が必要なときだけ高努力を許容

## 7. 追加アイデア（実装優先度つき）
### P0（短期）
- schema不一致時の回復条件を `length` 以外にも拡張（1回限定）
- 応答文の冗長化を防ぐ明示制約をプロンプトに追加
- Pre-flight で有効 `maxTokens` の不一致を即Fail

### P1（中期）
- 会話フェーズ推定器（quick/standard/deep）を導入
- プロファイル別に `timeout/retry` を適用
- エラー種別別の再試行戦略（429優先、401/403は即失敗）

### P2（中長期）
- deep のみ Responses API + reasoningモデルへルーティング
- reasoning effort のA/B（low/medium/high）
- Prompt cache活用で反復プロンプトの遅延とコストを削減

## 8. 次回A/B設計（推奨）
- 比較案:
  - A: 現行（900固定）
  - B: 動的（900既定 + recoveryで1500）
  - C: 1200固定（中間）
- サンプル:
  - 各条件 n>=30 会話（会話タイプを均等化）
- 主要KPI:
  - `AI_RESPONSE_INVALID_JSON` 率
  - `AI_RESPONSE_INVALID_SCHEMA` 率
  - `Wait > 15s` 率
  - p50/p95 完了時間
  - 1会話あたり総コスト
- 合格基準（例）:
  - エラー率 20%以上改善
  - p95劣化 10%以内
  - コスト増 10%以内

## 9. 公式仕様メモ（運用解釈）
- Structured Outputs（json_schema）は JSON mode より推奨です。
- JSON mode は指示不備時に空白出力が続く既知注意があります。
- Chat Completions の `max_tokens` は将来的には `max_completion_tokens` 移行に留意が必要です（特に o-series 互換）。
- reasoning effort は reasoningモデル利用時に有効です。

## 10. 参照
- 実測:
  - `reports/real-cost/LIVE/real-cost-LIVE-2026-02-11T13-08-40-132Z.md`
  - `reports/real-cost/LIVE/real-cost-LIVE-2026-02-11T13-10-24-467Z.md`
  - `reports/real-cost/LIVE/real-cost-LIVE-2026-02-11T13-15-31-610Z.md`
  - `reports/real-cost/LIVE/real-cost-LIVE-2026-02-11T13-17-26-402Z.md`
- 実装:
  - `workers/routes/chat.ts`
  - `workers/lib/openai.ts`
- OpenAI公式:
  - Reasoning models: https://platform.openai.com/docs/guides/reasoning
  - Structured Outputs: https://platform.openai.com/docs/guides/structured-outputs
  - Chat Completions API: https://developers.openai.com/api/reference/chat/create
  - Responses API: https://developers.openai.com/api/reference/responses/create
  - Chat→Responses移行: https://developers.openai.com/api/docs/guides/latest-model/

